\documentclass[sigconf]{acmart}
% Remove/adjust for your submission venue as needed.
% \settopmatter{printacmref=false} % Removes ACM Reference Format
% \renewcommand\footnotetextcopyrightpermission[1]{} % removes copyright text

\usepackage{lmodern}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{url}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning}


%\title{Towards Secure, Multitenant, Vendor-Neutral Enterprise RAG on Shared Infrastructure}
\title{Securing the Agent: Vendor-Neutral, Multitenant Enterprise Retrieval and Tool Use}

\author{Varsha Prasad}
\affiliation{%
  \institution{Red Hat AI}
  \city{Boston}
  \country{USA}
}
\email{vnarsing@redhat.com}

\author{Francisco Javier Arceo}
\affiliation{%
  \institution{Red Hat AI}
  \city{Boston}
  \country{USA}
}
\email{farceo@redhat.com}

\begin{abstract}
Retrieval-Augmented Generation (RAG) and agentic AI systems are increasingly prevalent in enterprise AI deployments. However, real enterprise environments introduce challenges largely absent from academic treatments and consumer-facing APIs: multiple tenants with heterogeneous data, strict access-control requirements, regulatory compliance, and cost pressures that demand shared infrastructure.

A fundamental problem underlies existing RAG architectures in these settings: retrieval systems rank documents by relevance---whether through semantic similarity, keyword matching, or hybrid approaches---not by authorization, so a query from one tenant can surface another tenant's confidential data simply because it scores highest. We formalize this gap and analyze additional shortcomings---including tool-mediated disclosure, context accumulation across turns, and client-side orchestration bypass---that arise when agentic systems conflate relevance with authorization. To address these challenges, we introduce a layered isolation architecture combining policy-aware ingestion, retrieval-time gating, and shared inference, enforced through server-side agentic orchestration. This approach centralizes security-critical operations---tool execution authorization, state isolation, and policy enforcement---on the server, creating natural enforcement points for multitenant isolation while allowing client-side frameworks to retain control over agent composition and latency-sensitive operations.

We validate the proposed architecture through an open sourced implementation in Llama Stack---a vendor-neutral framework realizing the Responses API paradigm with server-side multi-turn orchestration---demonstrating that secure multitenancy, cost-efficient resource sharing, and autonomous agent capabilities are simultaneously achievable on shared infrastructure.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010147.10010257</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10002951.10003227.10003351</concept_id>
  <concept_desc>Information systems~Data management systems</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[300]{Computing methodologies~Machine learning}
\ccsdesc[100]{Information systems~Data management systems}

\keywords{multitenancy, retrieval-augmented generation, access control, agentic AI, server-side orchestration, Llama Stack, LLM systems, LLMOps}


\DeclareMathOperator{\relevance}{relevance}

\begin{document}
\maketitle

\section{Introduction}
\subsection{Motivation}
Enterprise adoption of generative AI has evolved beyond simple prompt-response interactions toward agentic systems---AI applications that autonomously reason, use tools, retrieve information, and execute multi-step workflows to accomplish complex tasks~\cite{yao2023react,schick2023toolformer,karpas2022mrkl}.
This evolution reflects a fundamental shift: rather than treating large language models (LLMs) as sophisticated text generators, organizations now deploy them as reasoning engines capable of taking actions in the world.

An API-first paradigm is emerging to unify multi-turn inference, tool use, and retrieval behind a single endpoint.
OpenAI's Responses API is one prominent example, providing a unified interface for chat-style inference, tool invocation, retrieval, and stateful workflows~\cite{openaiResponsesAPI}, while open-source frameworks increasingly expose OpenAI-compatible endpoints to decouple applications from any single provider.

However, real-world enterprise deployments differ sharply from the assumptions embedded in consumer-facing APIs and many academic prototypes, exhibiting characteristics that demand specialized architectural consideration:
\begin{itemize}
  \item \textbf{Multiple tenants:} distinct business units, customers, or partners served from shared infrastructure, with strict isolation requirements.
  \item \textbf{Heterogeneous data:} document collections vary in format, sensitivity classification, and access requirements.
  \item \textbf{Strict access control:} regulatory frameworks require fine-grained governance with auditable access patterns.
  \item \textbf{Operational control:} visibility into agent behavior, tool execution sequences, and data access patterns is required for debugging and compliance, consistent with lessons from production ML engineering~\cite{amershi2019se4ml}.
  \item \textbf{Vendor independence:} lock-in to a single AI provider creates business risk; enterprises require on-prem and hybrid options.
\end{itemize}

Na\"{\i}ve approaches to addressing these requirements replicate the entire agentic stack per tenant: separate vector stores, dedicated inference endpoints, and isolated tool configurations.
This strategy incurs substantial costs---infrastructure scales linearly with tenants rather than with actual usage---and creates operational fragmentation that amplifies common ML systems maintenance risks~\cite{sculley2015hiddentechnicaldebt}.

\subsection{Problem statement}
This paper addresses a fundamental tension in enterprise agentic AI deployment:

\begin{quote}
Autonomous agents require flexible tool access and multi-turn reasoning capabilities, yet enterprise environments demand strict tenant isolation and policy enforcement---requirements that existing agentic architectures cannot simultaneously satisfy.
\end{quote}

Standard agentic AI deployments exhibit security assumptions incompatible with enterprise multitenancy:
\begin{enumerate}
  \item \textbf{Client-side orchestration:} the application manages the inference---tool---inference loop, distributing security-critical logic to potentially untrusted clients and increasing operational complexity~\cite{amershi2019se4ml}.
  \item \textbf{Homogeneous data access:} retrieval stacks assume uniform access to a corpus; retrieval methods (whether dense, sparse, or hybrid) optimize relevance ranking rather than authorization~\cite{karpukhin2020dpr}.
  \item \textbf{Implicit trust boundaries:} tool execution is often treated as a capability extension without systematic verification of who may invoke tools or consume tool outputs, despite the centrality of tool use in modern agent designs~\cite{yao2023react,schick2023toolformer,karpas2022mrkl}.
  \item \textbf{Stateless isolation:} requests are treated independently, ignoring how conversation state and cached tool results can leak across boundaries; such hidden couplings are a classic source of ML systems fragility~\cite{sculley2015hiddentechnicaldebt}.
\end{enumerate}

In multitenant settings, these assumptions create serious vulnerabilities.
A document highly similar to a query may belong to a different tenant.
A tool call may access resources outside the user's authorization scope.
Conversation history may accumulate context that crosses security boundaries.

This paper focuses on agentic RAG---the intersection of retrieval-augmented generation and autonomous tool use---as the primary case study, since retrieval is where the relevance-authorization gap is most acute. However, the layered isolation architecture and server-side enforcement patterns we propose are general and extend to other agentic capabilities including inference, tool execution, code generation, and multi-step workflows.

\subsection{Contributions}
This paper makes the following contributions:
\begin{enumerate}
  \item We formalize the problem of \emph{multitenant enterprise RAG} under shared infrastructure, showing why relevance ranking alone (whether vector-based, keyword-based, or hybrid) is insufficient to enforce isolation and authorization without explicit authorization predicates~\cite{karpukhin2020dpr,johnson2017faiss}.

  \item We analyze shortcomings of existing RAG and agentic systems in multitenant settings, including cross-tenant retrieval leakage, unauthorized context construction, and policy violations arising from client-side orchestration.

  \item We propose a layered isolation architecture for enterprise RAG that combines policy-aware ingestion, retrieval-time gating, and shared inference to achieve strict tenant isolation without per-tenant duplication of storage or models.

  \item We introduce server-side orchestration as a unifying enforcement layer that centralizes retrieval, tool execution, and state management, reducing the trusted computing base for secure multitenant RAG and aligning with production ML engineering best practices~\cite{sculley2015hiddentechnicaldebt,amershi2019se4ml}.

  \item We validate the proposed architecture through an implementation in Llama Stack~\cite{llamastack}, an open-source, vendor-neutral framework with Kubernetes deployment via an operator~\cite{llamastackk8soperator,burns2016borg}, demonstrating the feasibility of secure multitenant agentic RAG on shared infrastructure with pluggable models, vector stores, and tools.
\end{enumerate}

\paragraph{Primary sources.}
Llama Stack~\cite{llamastack}: \url{https://github.com/llamastack/}.\\
Kubernetes operator~\cite{llamastackk8soperator}: \url{https://github.com/llamastack/llama-stack-k8s-operator}.

\section{Background}
\subsection{The evolution of LLM application architectures}
LLM application architectures have evolved through distinct phases, each introducing new capabilities and security considerations.
\textbf{Completion APIs} exposed simple prompt\,$\rightarrow$\,text interfaces with perimeter-oriented security.

\textbf{Retrieval-augmented generation (RAG)}~\cite{lewis2020rag} introduced retrieval and new attack surfaces such as retrieval manipulation and context poisoning, building on foundational work on prompt injection vulnerabilities~\cite{willison2022prompt}.
\textbf{Dense retrieval and learned retrievers} (e.g., DPR~\cite{karpukhin2020dpr} and retrieval-augmented pretraining such as REALM~\cite{guu2020realm}) established the core technical basis for modern RAG.
\textbf{Retrieval infrastructure} evolved to support multiple search modalities: dense vector search across diverse vector databases~\cite{johnson2017faiss}, sparse keyword matching (BM25), and hybrid approaches combining both with neural rerankers~\cite{blendedrag2024}. However, all of these modalities optimize for relevance; none enforce authorization natively.
\textbf{Tool-using agents}~\cite{yao2023react,schick2023toolformer,karpas2022mrkl} extended LLMs with tool calls and an inference\,$\rightarrow$\,tool\,$\rightarrow$\,inference loop.
\textbf{Autonomous multi-step agents} execute multi-tool workflows with limited human oversight.
\textbf{The Responses API paradigm}~\cite{openaiResponsesAPI,openresponses} represents a convergence of these phases, unifying inference, tool use, retrieval, and state management behind a single endpoint.

Each phase introduced new capabilities but also inherited the security gaps of prior phases. The Responses API paradigm unifies these capabilities under a single interface but assumes single-tenant deployment. Enterprise multitenancy requires policy-aware tool execution, stateful conversation management, and orchestration controls---challenges amplified in agentic deployments where reasoning, retrieval, and tool execution interleave autonomously~\cite{sculley2015hiddentechnicaldebt,amershi2019se4ml}.

\section{Multitenant Agentic AI: Challenges and Solutions}
\label{sec:challenges-solutions}

We formalize the multitenant agentic AI environment: tenants ($T$) share infrastructure, each with associated data, users, tools, and policies. Agent execution follows the tool-using pattern~\cite{yao2023react,karpas2022mrkl}, where an execution sequence $E$ consists of alternating inference steps $i$, tool calls $\phi$, and responses $r$:

\begin{equation}
E = [(i_1, \phi_1, r_1), (i_2, \phi_2, r_2), \dots, (i_n, \emptyset, r_n)]
\end{equation}

where the final step has no tool call ($\emptyset$) and terminates with response $r_n$.

Standard agentic deployments exhibit several security assumptions that are fundamentally incompatible with enterprise multitenancy.

\subsection{Key security challenges}

\textbf{Relevance-authorization gap:} Retrieval systems---whether vector-based, keyword-based, or hybrid---optimize for relevance metrics rather than authorization policies. This creates a fundamental gap: search ranking considers semantic similarity, term frequency, or combined relevance signals, but authorization decisions depend on access control policies that are orthogonal to these relevance measures.

For tenants $T_A$ and $T_B$ sharing corpus $D = D_A \cup D_B$, retrieval methods cannot enforce tenant isolation without an authorization predicate. Let $q$ denote a query, $u$ denote a user, $d$ denote a document, $\theta$ denote a relevance threshold, and $P(u,d)$ denote an authorization policy that returns $\mathrm{permit}$ or $\mathrm{deny}$. Then secure retrieval requires:

\begin{equation}
\{ d \in D : \operatorname{relevance}\bigl(q,d\bigr) > \theta \wedge P\bigl(u,d\bigr) = \mathrm{permit} \}
\end{equation}

\textbf{Tool-mediated disclosure:} Agents invoke tools with agent credentials rather than end-user authorization, potentially accessing unauthorized data across tenant boundaries.

\textbf{Context accumulation:} Multi-turn conversations persist context without per-turn policy re-validation, enabling cross-tenant data leakage.

\textbf{Client-side bypass:} When orchestration runs client-side, malicious clients can skip authorization checks, manipulate tool invocation, or extract unauthorized data.

These shortcomings stem from distributing security-critical logic outside the trust boundary~\cite{sculley2015hiddentechnicaldebt,amershi2019se4ml,shapira2025agentsofchaos}.

\subsection{Proposed solution: layered isolation with server-side orchestration}
To address these shortcomings, we propose a two-part solution.
A three-layer isolation architecture secures the \emph{data path}: how documents are ingested, retrieved, and fed to the model.
Server-side orchestration secures the \emph{control path}: how tools are invoked, state is managed across turns, and policies are enforced (detailed in Section~\ref{sec:server-side}).
The three data-path layers are:

\textbf{Layer 1: Policy-aware ingestion.} Tenant metadata attached at ingestion: $\mathcal{I}(d, t) \rightarrow D_t$ tags document $d$ with tenant $t$'s attributes.

\textbf{Layer 2: Retrieval gating.} Two-tier enforcement: resource-level authorization before search and chunk-level filtering after retrieval, composing similarity search with authorization predicates.

\textbf{Layer 3: Shared inference.} LLM layer shared across tenants with isolation enforced at input construction, reducing cost from $O(N \cdot M)$ to $O(M)$.

The following subsections detail each layer.

\subsection{Policy-aware ingestion (Layer 1)}
Tenant metadata must be attached at document ingestion time, not retrofitted.
This reduces the risk of accidental cross-tenant coupling and missing invariants during system evolution~\cite{sculley2015hiddentechnicaldebt}.
The ingestion function $\mathcal{I}$ defined above ensures that every chunk inherits ownership metadata, so downstream retrieval and authorization can operate on consistent tenant attributes.

\subsection{Retrieval gating (Layer 2)}
Retrieval is gated in two tiers across multiple search modalities.
\textbf{(1) Resource-level attribute-based access control (ABAC):} before any search, the system checks that the user is authorized to read the search store; if not, no search is performed.
\textbf{(2) Chunk-level metadata filtering:} after retrieval, structured filters are applied on chunk metadata so that only documents satisfying the policy are admitted.
This two-tier gating applies regardless of search modality (dense, sparse, or hybrid): each retrieval path enforces authorization before applying relevance ranking, implementing the secure retrieval formulation from Equation~(2).
Where the backend supports it, predicate pushdown can enforce filtering efficiently at scale; otherwise, post-retrieval gating still enforces the relevance/authorization separation.

\subsection{Shared inference (Layer 3)}
The LLM inference layer is shared across tenants; the model itself does not require per-tenant isolation, only the context fed to it.
Because layers 1 and 2 ensure that only authorized documents and tool results enter the prompt, the inference layer can be safely shared.
At the serving layer, modern systems show that batching, scheduling, and memory management dominate throughput and latency for generative transformers~\cite{yu2022orca,kwon2023vllm}.
With $N$ tenants and $M$ model endpoints, cost scales as $O(M)$ rather than $O(N \cdot M)$.

\section{Server-Side Orchestration as Enforcement Layer}
\label{sec:server-side}

As introduced in Section~\ref{sec:challenges-solutions}, the control path---tool invocation, state management, and policy enforcement---benefits from server-side orchestration to reduce the trusted computing base (TCB).
This section examines the limitations of purely client-side orchestration for multitenant settings, the trade-offs of server-side orchestration, and how hybrid approaches may combine the strengths of both.

\subsection{Limitations of client-side orchestration}
Client-side orchestration offers important advantages: low latency for local tool calls, flexibility in composing custom agent logic, and the ability to leverage rich client-side frameworks. However, in multitenant enterprise settings, purely client-side patterns introduce specific shortcomings.
When the client controls the inference---tool---inference loop, a compromised or buggy client can skip retrieval filters, invoke unauthorized tools, or accumulate cross-tenant context.
Client-side orchestration expands the TCB to include untrusted client code, so server-side security invariants cannot be enforced from the server alone.
This is consistent with broader findings that ML systems become fragile when critical invariants are distributed across components without clear ownership or enforcement points~\cite{sculley2015hiddentechnicaldebt,amershi2019se4ml}.

\subsection{Trade-offs of server-side orchestration}
Server-side orchestration centralizes policy enforcement but introduces its own trade-offs: added latency for tool calls that could execute locally, reduced flexibility for client-specific agent logic, and increased server-side complexity. For workloads where all tenants are trusted and isolation is not a concern, client-side orchestration remains a pragmatic and effective choice.

In practice, hybrid architectures are likely to emerge as the dominant pattern: server-side orchestration enforces security-critical invariants (authorization, state isolation, audit logging), while client-side frameworks retain control over agent composition, user interaction, and latency-sensitive tool calls that do not cross trust boundaries. Llama Stack's design supports this model---clients orchestrate their agent logic using any OpenAI-compatible framework, while the server enforces policy at the API boundary.

\begin{figure}[!htbp]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tikzpicture}[
      box/.style={draw, rounded corners, align=center, inner sep=3pt, font=\small},
      arrow/.style={-{Stealth[length=2.0mm]}, thick}
    ]
      \node[box] (req) {Request};
      \node[box, right=6mm of req] (in) {Input\\safety};
      \node[box, right=6mm of in] (inf) {Inference};
      \node[box, right=6mm of inf] (tool) {Tool\\execution};
      \node[box, right=6mm of tool] (out) {Output\\safety};
      \node[box, right=6mm of out] (store) {Response\\storage};
      \draw[arrow] (req) -- (in);
      \draw[arrow] (in) -- (inf);
      \draw[arrow] (inf) -- (tool);
      \draw[arrow] (tool) -- (out);
      \draw[arrow] (out) -- (store);
    \end{tikzpicture}%
    }
    \caption{Server-side orchestration flow: every step runs inside the server trust boundary.}
    \label{fig:orchestration-flow}
  \end{figure}

\subsection{Enforcement points}
Table~\ref{tab:enforcement} maps each shortcoming from Section~\ref{sec:challenges-solutions} to the enforcement point that mitigates it.

\begin{table}[!htbp]
  \centering
  \small
  \setlength{\tabcolsep}{3pt}
  \renewcommand{\arraystretch}{1.1}
  \begin{tabularx}{\columnwidth}{@{}l|X@{}}
  \toprule
  Shortcoming & Enforcement point \\
  \midrule
  Cross-tenant retrieval leakage & Layer 2 retrieval gating (ABAC + metadata filters) \\
  \midrule
  Context accumulation & Tenant-scoped state storage and per-turn authorization \\
  \midrule
  Tool-mediated disclosure & Server-side tool execution with authorization propagation \\
  \midrule
  Client-side bypass & Server-side orchestration (reduced TCB) \\
  \midrule
  Audit failure & Server-side telemetry and tracing \\
  \bottomrule
  \end{tabularx}
  \caption{Mapping from shortcomings to architectural enforcement points.}
  \label{tab:enforcement}
\end{table}

\section{Llama Stack Implementation}
\label{sec:implementation}

Llama Stack~\cite{llamastack} provides an open-source implementation of the architecture proposed in Sections~\ref{sec:challenges-solutions} and~\ref{sec:server-side}. The framework executes the complete agentic control loop server-side via the Responses API~\cite{openaiResponsesAPI}, with pluggable providers for inference (vLLM~\cite{kwon2023vllm}, Ollama, OpenAI), vector stores (Chroma, pgvector, Elasticsearch), and tools (\texttt{file\_search}, \texttt{web\_search}, Model Context Protocol~\cite{mcp}). Together with open models such as gpt-oss~\cite{openai2025gptoss120bgptoss20bmodel}, this provides a complete open-source alternative to proprietary agentic AI platforms. The Kubernetes Operator~\cite{llamastackk8soperator} automates deployment across multiple topologies, from shared instances with logical isolation to per-tenant deployments with namespace isolation.

\subsection{Layered Architecture}
Llama Stack follows a layered architecture that separates concerns across distinct tiers, as illustrated in Figure~\ref{fig:architecture}. At the top, an HTTP/REST layer handles request routing, authentication, quota enforcement, and streaming. Beneath this layer, a set of domain-specific APIs expose functionality for inference, agentic execution, vector I/O, safety, and tool integration.

A routing layer mediates between API calls and concrete provider implementations, resolving logical resource identifiers (e.g., model identifiers or vector store identifiers) to physical provider instances. Below this, a provider layer encapsulates both inline providers executing in-process and remote providers that adapt external services. Persistent state is maintained in a storage layer comprising key--value stores, relational stores, and vector databases.

This separation enables independent evolution of APIs, providers, and storage backends while preserving a unified control plane for policy enforcement.

\subsection{Core APIs and Agentic Execution}
Llama Stack defines a comprehensive set of APIs covering the full lifecycle of agentic applications, including inference, agents, vector I/O, safety, tools, file management, and evaluation. Each API is designed with multitenancy as a first-class concern, enabling tenant-scoped resource management and access control.

The Agents API, which implements the OpenAI Responses API paradigm, is particularly significant for multitenant agentic systems. Unlike traditional chat completion APIs that terminate after a single inference call, the Responses API orchestrates complete agentic workflows. A single request may trigger multiple inference calls, tool executions, safety checks, and state transitions before producing a final response.

All such operations are executed within the server boundary. Conversation state is retrieved and persisted server-side, tools are invoked under centralized authorization, and safety guardrails are applied at each step. This design ensures that intermediate context, tool outputs, and execution state remain subject to uniform access control policies.

As discussed in Section~\ref{sec:server-side}, Llama Stack operates as a \emph{platform layer}---the server-side execution target that client-side agent frameworks (LangChain~\cite{langchain}, LangGraph~\cite{langgraph}, CrewAI~\cite{crewai}, and others) call into via OpenAI-compatible endpoints, gaining server-side policy enforcement, multitenancy, and provider portability without changes to agent code.

\subsection{Provider Architecture}
Extensibility in Llama Stack is achieved through its provider architecture. Each API may be backed by multiple providers, which are categorized as inline or remote. Inline providers execute within the Llama Stack process and are suitable for sensitive operations requiring in-process execution. Remote providers adapt external inference engines, vector databases, or services through standardized interfaces.

This separation enables hybrid deployments in which sensitive data paths remain local while computationally intensive operations are delegated to scalable external services. Crucially, provider substitution is transparent to clients, as all interactions occur through the unified API layer. A developer can prototype locally with inline providers (e.g., sqlite-vec for vector search, an in-process safety model), then move to production-grade remote providers (e.g., pgvector, vLLM) without changing application code or API calls---only the distribution configuration changes. This lowers the barrier from experimentation to production deployment while preserving consistent security and isolation guarantees across environments.

\subsection{Routing Layer}
The routing layer dispatches API requests to provider instances based on logical resource identifiers. For example, inference requests are routed according to model identifiers, while vector queries are routed based on vector store identifiers. This indirection enables fine-grained control over resource access and placement.

From a multitenancy perspective, the routing layer serves as a critical enforcement point. Routing decisions can incorporate authorization checks, tenant identity, and policy constraints before delegating requests to providers. Different tenants may thus be routed to distinct provider instances or storage backends while sharing the same API surface.

\subsection{Distribution Model}
A distribution packages a specific set of APIs, provider configurations, and registered resources into a deployable unit. Distributions support turnkey deployment for common scenarios, environment-specific configuration (e.g., development versus production), and seamless provider substitution without application changes.

By decoupling application logic from provider selection, the distribution model enables organizations to evolve their infrastructure and vendor choices while preserving stable interfaces for agentic applications.

\subsection{Access Control Framework}
Llama Stack includes a declarative, policy-based access control framework that evaluates authorization decisions at runtime. Access rules specify permit or forbid scopes with optional conditions based on ownership and attribute matching. The default policy permits access when the user is the resource owner or when the user's attributes (roles, teams, projects, namespaces) match the resource's access attributes. A default-deny model ensures that access is only granted when explicitly permitted by policy.

Authorization is enforced at three levels: API route middleware, routing table resolution (before resolving a vector store or model to a provider), and storage read time, where query filters are constructed from the current user's attributes so that tenants only see their own or attribute-matched rows. JWT or Kubernetes authentication providers map external identity claims into these attributes, so enterprise identity systems can drive isolation without embedding tenant IDs in application logic.

\subsection{Server Architecture}
The Llama Stack server is implemented atop a modern asynchronous web framework and provides OpenAI-compatible endpoints for inference and agentic execution. Authentication middleware supports pluggable identity providers, while quota management enables per-principal rate limiting and usage tracking. Streaming execution is supported for long-running agentic workflows, and telemetry integration enables end-to-end observability.

Importantly, all agentic orchestration occurs within the server process, enabling comprehensive audit logging of inference calls, tool executions, and data access events.

\begin{figure}[!htbp]
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tikzpicture}[
    box/.style={draw, rounded corners, align=center, inner sep=4pt, minimum height=8mm, font=\small},
    arrow/.style={-{Stealth[length=2.0mm]}, thick}
  ]
    \node[box] (client) {Client\\\texttt{POST /v1/responses}};
    \node[box, below=7mm of client] (auth) {AuthN/AuthZ\\tenant context};
    \node[box, below=7mm of auth] (orch) {Server-side orchestrator\\(multi-turn loop)};

    \node[box, below left=7mm and 12mm of orch] (infer) {Shared inference\\(LLM serving)};
    \node[box, below=7mm of orch] (vec) {Vector store\\(logical isolation)};
    \node[box, below right=7mm and 12mm of orch] (policy) {Policy engine\\(ABAC/RBAC)};

    \node[box, below=12mm of vec] (k8s) {Kubernetes substrate};
    \node[box, below=7mm of k8s] (op) {K8s operator\\(CRDs, reconcile)};

    \draw[arrow] (client) -- (auth);
    \draw[arrow] (auth) -- (orch);

    \draw[arrow] (orch) -- (infer);
    \draw[arrow] (orch) -- (vec);
    \draw[arrow] (orch) -- (policy);

    \draw[arrow] (op) -- (k8s);
    \draw[arrow] (k8s) -- (infer);
    \draw[arrow] (k8s) -- (vec);
  \end{tikzpicture}%
  }
  \caption{Llama Stack architecture for multitenant enterprise agentic AI on shared Kubernetes infrastructure.}
  \label{fig:architecture}
\end{figure}


\subsection{Responses API Implementation}
Llama Stack implements the Responses API not only as an agentic execution endpoint but as a resource model encompassing vector stores, files, and conversations---each a first-class, tenant-scoped API object subject to the same ABAC policies as responses themselves.
The core endpoint supports creating responses (executing an agent), retrieving, listing, inspecting input items, and deleting responses, in both streaming and non-streaming modes.
Internally, the implementation converts request input to chat messages and runs the inference---tool loop via a streaming orchestrator or a synchronous path. Built-in tools (\texttt{file\_search}, \texttt{web\_search}, MCP) are resolved and executed server-side during this loop.
Results are persisted through a responses store backed by an authorized SQL layer so that rows are tagged with owner and access attributes and filtered on read by the default ABAC policy.
Streaming events expose reasoning, tool calls, and text deltas, giving operators and auditors fine-grained visibility into agent behavior without requiring client-side instrumentation.

\subsection{Search abstraction and filters}
Llama Stack defines a search abstraction layer that provides uniform access to multiple retrieval modalities: vector databases (ChromaDB, PGVector, Elasticsearch, Qdrant, Weaviate, Milvus, Oracle Cloud Infrastructure, sqlite-vec), keyword search engines, and hybrid pipelines with optional reranking. Advanced chunking strategies (including contextual enrichment) are supported at ingestion time.
Each search store is a logical resource registered in a routing table and subject to the same access control as other resources; creation assigns an owner from the authenticated user.
The abstraction supports structured filtering through metadata predicates applied consistently across all search approaches: comparison operators (\texttt{eq}, \texttt{ne}, \texttt{gt}, \texttt{gte}, \texttt{lt}, \texttt{lte}) and compound \texttt{and}/\texttt{or} filters.
When the agent uses \texttt{file\_search}, the server applies the user's tenant and policy attributes so that retrieval is gated by both resource-level and chunk-level checks, enabling metadata-driven isolation without requiring a separate physical index per tenant.

\subsection{Cloud-Native Deployment}
The architectural guarantees described above must be realized on shared infrastructure. Kubernetes has become the de facto substrate for enterprise deployments~\cite{burns2016borg}.

The Llama Stack Kubernetes Operator~\cite{llamastackk8soperator} automates deployment and lifecycle management through custom resources that declaratively specify server configurations, backend connections, and isolation policies. The operator supports multiple deployment topologies:

\begin{itemize}
  \item \textbf{Shared instances}: A single deployment serves multiple tenants with application-level ABAC isolation. For example, multiple business units in a regulated organization share one Llama Stack instance, avoiding per-unit infrastructure costs while satisfying compliance requirements.
  \item \textbf{Per-tenant instances}: Separate deployments in tenant-specific namespaces combine Kubernetes RBAC with application-level policies. A platform team offering AI-as-a-service internally can provision per-team distributions, combining namespace-level network isolation with ABAC for defense in depth.
  \item \textbf{Hybrid approaches}: High-security tenants receive dedicated instances while lower-sensitivity tenants share infrastructure.
\end{itemize}

In all topologies, the provider abstraction allows organizations to start with lightweight backends (e.g., Ollama, sqlite-vec) during development and migrate to production-grade infrastructure (e.g., vLLM~\cite{kwon2023vllm}, pgvector) without modifying agent code or security policies---only the distribution configuration changes.

The operator's provider abstraction enables deployment of heterogeneous inference backends (vLLM~\cite{kwon2023vllm}, Ollama, proprietary endpoints) and vector databases (FAISS~\cite{johnson2017faiss}, ChromaDB, PGVector, Elasticsearch, Qdrant, Weaviate, Milvus, Oracle Cloud Infrastructure, sqlite-vec) as shared Kubernetes services. Multiple Llama Stack instances reference the same backends, achieving cost savings through infrastructure sharing while maintaining logical isolation through the authorization layer.

This design scales economically: adding tenants requires only logical configuration (authorization policies, database schemas) rather than full infrastructure duplication, crucial for cost-constrained enterprise deployments.

\section{Analysis and Discussion}

\subsection{Design trade-offs and limitations}
The architecture involves several trade-offs:

\begin{itemize}
  \item \textbf{ABAC policy complexity.} Policy complexity grows with the number of tenants, roles, and resource types. Organizations with deeply nested permission structures may face policy management overhead.

  \item \textbf{Metadata filtering performance.} Filtering performance varies across vector database backends: some support predicate pushdown natively, while others require post-retrieval filtering that adds latency on large corpora.

  \item \textbf{Model prior knowledge.} The inference layer shares models across tenants, which reduces cost but means that model prior knowledge---information learned during pretraining---is orthogonal to RAG isolation and cannot be controlled by the authorization layer.

  \item \textbf{Client-side function tools.} Client-side function tools, by design, execute outside the server trust boundary. Llama Stack mitigates this through explicit tool classification but cannot enforce server-side invariants on client-executed code.
\end{itemize}

\subsection{Related work}

Several systems aim to standardize LLM-based agentic applications. The closest to our work are API-layer platforms and enterprise agent runtimes.

OpenAI's Responses API~\cite{openaiResponsesAPI}, function-calling conventions~\cite{openaiFunctionCalling}, and the Model Context Protocol (MCP)~\cite{mcp} establish \emph{what} an agent can do but are silent on \emph{who} may do it---they assume a single-tenant caller. Databricks' Mosaic AI Agent Framework~\cite{databricksAgentFramework} wraps agents in an MLflow ResponsesAgent~\cite{mlflow,databricksResponsesAgent} with managed MCP tools and Unity Catalog governance, providing the closest enterprise parallel, but couples the experience to the Databricks stack. Llama Stack adopts the same interface conventions while providing multitenant isolation through an open, vendor-neutral API layer deployable on any infrastructure.

LLM serving engines such as vLLM~\cite{kwon2023vllm} and Orca~\cite{yu2022orca} optimize inference throughput but are agnostic to tenant isolation; vector databases such as Milvus~\cite{milvus} and Weaviate~\cite{weaviate} and platforms such as Vectara~\cite{vectara} rank by relevance without authorization enforcement. Llama Stack treats these as pluggable providers beneath its authorization layer. Agent orchestration frameworks---LangGraph~\cite{langgraph}, Microsoft's Agent Framework~\cite{microsoftAgentFramework} (successor to Semantic Kernel~\cite{semanticKernel} and AutoGen~\cite{autogen}), Google ADK~\cite{googleADK}, Haystack~\cite{haystack}, LlamaIndex~\cite{llamaindex}, Smolagents~\cite{smolagents}, and Pydantic AI~\cite{pydanticai}---provide developer-facing abstractions but orchestrate from the client side; Llama Stack serves as the server-side execution target these frameworks call into.

Production ML research has documented the fragility of distributed invariants~\cite{sculley2015hiddentechnicaldebt,amershi2019se4ml}, and lifecycle platforms such as MLflow~\cite{mlflow} address deployment but not agentic multitenancy. To our knowledge, no prior work addresses the intersection of standardized agentic API design, server-side orchestration, and multitenant isolation on shared infrastructure---the gap this paper aims to fill.

\section{Conclusion}
Enterprise deployment of agentic AI systems introduces security and compliance challenges that existing architectures---designed for single-tenant, consumer-facing use---do not address. This paper formalized the relevance-authorization gap in retrieval~\cite{karpukhin2020dpr,johnson2017faiss} and identified shortcomings that arise when agentic systems conflate relevance with authorization in multitenant settings.

To address these shortcomings, we proposed a layered isolation architecture that separates the data path (policy-aware ingestion, retrieval gating, shared inference) from the control path (server-side orchestration for tool execution, state management, and policy enforcement). Rather than positioning server-side orchestration as a replacement for client-side frameworks, we argued that hybrid architectures---where server-side enforcement secures trust-boundary-crossing operations while client-side frameworks retain control over agent composition---represent a practical path forward.

Our open-source implementation through Llama Stack~\cite{llamastack} demonstrates that the proposed architecture is realizable in a vendor-neutral framework with server-side Responses API execution~\cite{openaiResponsesAPI,openresponses} and pluggable providers. Combined with efficient serving infrastructure~\cite{kwon2023vllm} and open models~\cite{openai2025gptoss120bgptoss20bmodel}, this provides a complete open-source alternative to proprietary agentic platforms, enabling secure multitenancy on shared infrastructure without per-tenant duplication.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
